{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BPETokenizer:\n",
    "    \n",
    "#     def __init__(self, vocab_size=1000, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\"]):\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.special_tokens = special_tokens\n",
    "#         self.bpe_vocab = {}\n",
    "#         self.merges = []\n",
    "#         self.unk_token = \"[UNK]\"\n",
    "\n",
    "#     def get_vocab(self):\n",
    "#         return self.bpe_vocab\n",
    "\n",
    "#     def build_vocab(self, corpus):\n",
    "#         \"\"\"Build the BPE vocabulary based on the corpus.\"\"\"\n",
    "#         # Split words into characters, with space being a delimiter for words\n",
    "#         token_freqs = defaultdict(int)\n",
    "#         for sentence in corpus:\n",
    "#             words = sentence.strip().split()\n",
    "#             for word in words:\n",
    "#                 # Add spaces between characters and a word boundary symbol </w>\n",
    "#                 word = \" \".join(list(word)) + \" </w>\"\n",
    "#                 token_freqs[word] += 1\n",
    "\n",
    "#         # Build vocabulary by merging the most frequent pairs\n",
    "#         for _ in range(self.vocab_size):\n",
    "#             pairs = self.get_stats(token_freqs)\n",
    "#             if not pairs:\n",
    "#                 break\n",
    "\n",
    "#             # Get the most frequent pair\n",
    "#             best_pair = max(pairs, key=pairs.get)\n",
    "#             self.merges.append(best_pair)\n",
    "\n",
    "#             # Merge the best pair\n",
    "#             token_freqs = self.merge_vocab(best_pair, token_freqs)\n",
    "\n",
    "#         # Create final vocab mapping\n",
    "#         self.bpe_vocab = {word: idx for idx, word in enumerate(self.special_tokens + list(token_freqs.keys()))}\n",
    "\n",
    "#     def get_stats(self, token_freqs):\n",
    "#         \"\"\"Count frequency of token pairs in the vocabulary.\"\"\"\n",
    "#         pairs = defaultdict(int)\n",
    "#         for word, freq in token_freqs.items():\n",
    "#             tokens = word.split()\n",
    "#             for i in range(len(tokens) - 1):\n",
    "#                 pairs[(tokens[i], tokens[i + 1])] += freq\n",
    "#         return pairs\n",
    "\n",
    "#     def merge_vocab(self, pair, token_freqs):\n",
    "#         \"\"\"Merge the most frequent pair in the vocabulary.\"\"\"\n",
    "#         bigram = re.escape(\" \".join(pair))\n",
    "#         p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "#         new_vocab = {}\n",
    "#         for word in token_freqs:\n",
    "#             new_word = p.sub(\"\".join(pair), word)\n",
    "#             new_vocab[new_word] = token_freqs[word]\n",
    "#         return new_vocab\n",
    "\n",
    "#     def tokenize(self, text):\n",
    "#         \"\"\"Tokenize a sentence using the learned BPE vocabulary.\"\"\"\n",
    "#         words = text.strip().split()\n",
    "#         tokens = []\n",
    "#         for word in words:\n",
    "#             word = \" \".join(list(word)) + \" </w>\"\n",
    "#             tokens.extend(self.encode_word(word))\n",
    "#         return tokens\n",
    "\n",
    "#     def encode_word(self, word):\n",
    "#         \"\"\"Encode a single word using the learned BPE merges.\"\"\"\n",
    "#         tokens = word.split()\n",
    "#         for merge in self.merges:\n",
    "#             while \" \".join(merge) in \" \".join(tokens):\n",
    "#                 i = tokens.index(merge[0])\n",
    "#                 if i + 1 < len(tokens) and tokens[i + 1] == merge[1]:\n",
    "#                     tokens[i:i + 2] = [\"\".join(merge)]\n",
    "#         return tokens\n",
    "\n",
    "#     def convert_tokens_to_ids(self, tokens):\n",
    "#         \"\"\"Convert tokens to IDs using the BPE vocabulary.\"\"\"\n",
    "#         return [self.bpe_vocab.get(token, self.bpe_vocab[self.unk_token]) for token in tokens]\n",
    "\n",
    "#     def convert_ids_to_tokens(self, ids):\n",
    "#         \"\"\"Convert token IDs back to tokens.\"\"\"\n",
    "#         inv_vocab = {idx: token for token, idx in self.bpe_vocab.items()}\n",
    "#         return [inv_vocab.get(i, self.unk_token) for i in ids]\n",
    "\n",
    "#     def encode(self, text, add_special_tokens=True):\n",
    "#         \"\"\"Convert text to token IDs, optionally adding special tokens.\"\"\"\n",
    "#         tokens = self.tokenize(text)\n",
    "#         token_ids = self.convert_tokens_to_ids(tokens)\n",
    "        \n",
    "#         if add_special_tokens:\n",
    "#             token_ids = [self.bpe_vocab[\"[CLS]\"]] + token_ids + [self.bpe_vocab[\"[SEP]\"]]\n",
    "        \n",
    "#         return token_ids\n",
    "\n",
    "#     def decode(self, token_ids, skip_special_tokens=True):\n",
    "#         \"\"\"Convert token IDs back into text.\"\"\"\n",
    "#         tokens = self.convert_ids_to_tokens(token_ids)\n",
    "#         if skip_special_tokens:\n",
    "#             tokens = [t for t in tokens if t not in self.special_tokens]\n",
    "#         return \" \".join(tokens).replace(\" </w>\", \"\").replace(\" \", \"\")\n",
    "\n",
    "\n",
    "# with open(\"../data/shakespeare/input.txt\", \"r\") as file:\n",
    "#     corpus = file.readlines()\n",
    "\n",
    "# bpe_tokenizer = BPETokenizer(vocab_size=vocab_size)\n",
    "# bpe_tokenizer.build_vocab(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = 10770   # Vocabulary size (e.g., from tokenizer)\n",
    "embed_dim = 512      # Embedding dimension\n",
    "num_heads = 8        # Number of attention heads\n",
    "ff_hidden_dim = 2048 # Feedforward hidden dimension\n",
    "num_layers = 6       # Number of transformer decoder layers\n",
    "max_seq_len = 128    # Maximum sequence length\n",
    "num_epochs = 10      # Number of training epochs\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('mps')\n",
    "\n",
    "filepath = \"../data/shakespeare/input.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(CausalSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert (\n",
    "            self.head_dim * num_heads == embed_dim\n",
    "        ), \"Embedding dimension must be divisible by number of heads\"\n",
    "\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.scale = self.head_dim ** -0.5  # scaling factor for query\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "\n",
    "        # Compute Q, K, V\n",
    "        Q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        # Causal mask (upper triangular, prevents attending to future tokens)\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).to(x.device)\n",
    "        causal_mask = causal_mask.masked_fill(causal_mask == 1, float('-inf'))\n",
    "        attn_scores = attn_scores + causal_mask\n",
    "\n",
    "        # Attention probabilities\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # Weighted sum of values\n",
    "        attn_output = torch.matmul(attn_probs, V)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "\n",
    "        # Final output projection\n",
    "        return self.out_proj(attn_output)\n",
    "\n",
    "\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_hidden_dim, dropout=0.1):\n",
    "        super(TransformerDecoderBlock, self).__init__()\n",
    "        self.self_attn = CausalSelfAttention(embed_dim, num_heads)\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_dim, embed_dim),\n",
    "        )\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention + residual connection\n",
    "        attn_output = self.self_attn(x)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x = self.layernorm1(x)\n",
    "\n",
    "        # Feedforward + residual connection\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = x + self.dropout(ffn_output)\n",
    "        x = self.layernorm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CausalTransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, ff_hidden_dim, num_layers, max_seq_len):\n",
    "        super(CausalTransformerDecoder, self).__init__()\n",
    "        self.embed_tokens = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embed_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderBlock(embed_dim, num_heads, ff_hidden_dim)\n",
    "        for _ in range(num_layers)])\n",
    "        self.output_proj = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "\n",
    "        # Token + position embeddings\n",
    "        positions = torch.arange(0, seq_len, device=input_ids.device).unsqueeze(0)\n",
    "        x = self.embed_tokens(input_ids) + self.position_embedding(positions)\n",
    "\n",
    "        # Pass through layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Final linear projection to vocab size\n",
    "        logits = self.output_proj(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../data/shakespeare/input.txt\n",
      "  input_format: \n",
      "  model_prefix: ../data/shakespeare/sentencepiece\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 10770\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: ../data/shakespeare/input.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 32777 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=1108153\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9692% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=59\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999692\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 32777 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=563788\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 33662 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 32777\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 25670\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 25670 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12635 obj=11.7318 num_tokens=52726 num_tokens/piece=4.17301\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10764 obj=9.49501 num_tokens=53076 num_tokens/piece=4.93088\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: ../data/shakespeare/sentencepiece.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: ../data/shakespeare/sentencepiece.vocab\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, filepath, seq_len):\n",
    "        self.tokens = []\n",
    "        with open(filepath, \"r\") as file:\n",
    "            corpus = \"\".join(file.readlines())\n",
    "        model_prefix = os.path.join(os.path.dirname(filepath), \"sentencepiece\")\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            input=filepath,\n",
    "            model_prefix=model_prefix,\n",
    "            vocab_size=vocab_size,\n",
    "        )\n",
    "        self.sp = spm.SentencePieceProcessor(model_file=model_prefix + \".model\")\n",
    "        tokens = self.sp.EncodeAsIds(corpus)\n",
    "        for idx in range(0, len(tokens) - seq_len + 1):\n",
    "            self.tokens.append(tokens[idx:idx + seq_len])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.tokens[idx]\n",
    "        labels = input_ids[1:]\n",
    "        labels.append(self.sp.pad_id())\n",
    "        return torch.tensor(input_ids), \\\n",
    "               torch.tensor(labels)\n",
    "\n",
    "dataset = TextDataset(filepath=filepath, seq_len=max_seq_len)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CausalTransformerDecoder(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    ff_hidden_dim=ff_hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    max_seq_len=max_seq_len,\n",
    ")\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.sp.pad_id())\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(num_epochs):\n",
    "#     model.train()  # Set the model to training mode\n",
    "#     total_loss = 0  # Accumulate loss over the epoch\n",
    "\n",
    "#     for batch_idx, (input_ids, labels) in enumerate(dataloader):\n",
    "#         input_ids, labels = input_ids.to(device), labels.to(device)  # Move to device\n",
    "\n",
    "#         optimizer.zero_grad()  # Clear gradients\n",
    "\n",
    "#         # Forward pass\n",
    "#         logits = model(input_ids)  # Shape: (batch_size, seq_len, vocab_size)\n",
    "\n",
    "#         # Reshape logits and labels for the loss function\n",
    "#         logits = logits.view(\n",
    "#             -1, vocab_size\n",
    "#         )  # Shape: (batch_size * seq_len, vocab_size)\n",
    "#         labels = labels.view(-1)  # Shape: (batch_size * seq_len)\n",
    "\n",
    "#         # Compute loss\n",
    "#         loss = criterion(logits, labels)\n",
    "#         print(labels)\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         # Backward pass\n",
    "#         loss.backward()\n",
    "#         optimizer.step()  # Update parameters\n",
    "\n",
    "#         if batch_idx % 10 == 0:  # Print every 10 batches\n",
    "#             print(\n",
    "#                 f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx/len(dataloader)*100:.4f}%], Loss: {loss.item():.4f}\"\n",
    "#             )\n",
    "\n",
    "#     avg_loss = total_loss / len(dataloader)  # Average loss for the epoch\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalTransformerDecoder(\n",
       "  (embed_tokens): Embedding(10770, 512)\n",
       "  (position_embedding): Embedding(128, 512)\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x TransformerDecoderBlock(\n",
       "      (self_attn): CausalSelfAttention(\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (output_proj): Linear(in_features=512, out_features=10770, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalTransformerDecoder(\n",
       "  (embed_tokens): Embedding(10770, 512)\n",
       "  (position_embedding): Embedding(128, 512)\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x TransformerDecoderBlock(\n",
       "      (self_attn): CausalSelfAttention(\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (output_proj): Linear(in_features=512, out_features=10770, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('../data/shakespeare/model_weights.pth', weights_only=True, map_location=torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file=\"../data/shakespeare/sentencepiece.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:  ⁇ TON: And speaking it, he wistly look'd on me, And who should say, 'I would thou wert the man' That would divorce this terror from my heart;' Meaning the king at Pomfret. Come, let's go\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def top_k_sampling(logits, k):\n",
    "    \"\"\"\n",
    "    Perform top-k sampling on logits.\n",
    "    \n",
    "    Args:\n",
    "        logits: Tensor of shape (vocab_size,)\n",
    "        k: Number of top tokens to sample from.\n",
    "    \n",
    "    Returns:\n",
    "        next_token: The sampled token index.\n",
    "    \"\"\"\n",
    "    # Get top-k logits and their indices\n",
    "    top_k_logits, top_k_indices = torch.topk(logits, k)\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    top_k_probs = F.softmax(top_k_logits, dim=-1)\n",
    "\n",
    "    # Sample a token from the top-k probabilities\n",
    "    next_token = torch.multinomial(top_k_probs, num_samples=1)\n",
    "\n",
    "    # Map back to original vocabulary indices\n",
    "    return top_k_indices[next_token.item()]\n",
    "\n",
    "def sample_from_model(model, sp, max_length, start_sequence, k=10, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate a sequence from the trained transformer model using top-k sampling.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained transformer model.\n",
    "        sp: SentencePiece tokenizer.\n",
    "        max_length: Maximum sequence length.\n",
    "        start_sequence: Initial sequence to start the generation.\n",
    "        k: Number of top tokens to sample from (for top-k sampling).\n",
    "        temperature: Controls randomness in sampling; higher values make the output more random.\n",
    "\n",
    "    Returns:\n",
    "        generated_sequence: List of token IDs (generated sequence).\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    generated_sequence = start_sequence[:]\n",
    "    input_ids = torch.tensor(start_sequence, device=device).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through the model to get logits\n",
    "            logits = model(input_ids)  # Shape: (batch_size, seq_len, vocab_size)\n",
    "            \n",
    "            # Take logits of the last token in the sequence\n",
    "            logits = logits[:, -1, :] / temperature  # Shape: (batch_size, vocab_size)\n",
    "\n",
    "            # Apply top-k sampling to select the next token\n",
    "            next_token_id = top_k_sampling(logits.squeeze(), k)\n",
    "\n",
    "            # Stop if the model outputs an end token (e.g., using the pad token as end token)\n",
    "            if next_token_id == sp.pad_id():  # Assuming sp.pad_id() is the end token\n",
    "                break\n",
    "\n",
    "            # Append the new token to the generated sequence\n",
    "            generated_sequence.append(next_token_id.cpu().item())\n",
    "\n",
    "            # Update input for the next iteration\n",
    "            input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "\n",
    "    return generated_sequence\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `model` is your trained CausalTransformerDecoder model and `sp` is your SentencePiece tokenizer\n",
    "start_sequence = [sp.piece_to_id('[CLS]')]  # Replace with your actual start token\n",
    "\n",
    "# Generate a sequence using top-k sampling\n",
    "generated_sequence = sample_from_model(model, sp, max_length=50, start_sequence=start_sequence, k=10, temperature=1.0)\n",
    "\n",
    "# Decode the token IDs back to text\n",
    "generated_text = sp.decode(generated_sequence)\n",
    "print(f'Generated Text: {generated_text}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
