{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convolve(X, F):\n",
    "#     dim1 = X.shape[0]-F.shape[0]+1\n",
    "#     dim2 = X.shape[0]-F.shape[0]+1\n",
    "#     res = np.full([dim1, dim2], fill_value=np.nan)\n",
    "#     for i1 in range(dim1):\n",
    "#         for i2 in range(dim2):\n",
    "#             res[i1, i2] = np.sum(X[i1:i1+F.shape[0], i2:i2+F.shape[1]] * F)\n",
    "#             print(i1, i1+F.shape[0], i2, i2+F.shape[1])\n",
    "#     return res\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "train_images = np.squeeze(train_images)\n",
    "train_labels = np.squeeze(train_labels)\n",
    "test_images = np.squeeze(test_images)\n",
    "test_labels = np.squeeze(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "\n",
    "def D_ReLU(x):\n",
    "    return np.diag((x >= 0).astype(float))\n",
    "\n",
    "\n",
    "def SoftMax(x):\n",
    "    tmp = np.exp(x - np.max(x))\n",
    "    return tmp / np.sum(tmp)\n",
    "\n",
    "\n",
    "def D_SoftMax(x):\n",
    "    tmp = SoftMax(x)\n",
    "    return -1 * np.outer(tmp, tmp) + np.diag(tmp)\n",
    "\n",
    "\n",
    "def CategoricalCrossEntropy(y_pred, y_true):\n",
    "    tmp = np.clip(y_pred, 1e-9, 1-1e-9)\n",
    "    return np.mean(-1 * np.sum(y_true * np.log2(tmp) + (1 - y_true) * np.log2(1 - tmp), axis=1), axis=0)\n",
    "\n",
    "\n",
    "def D_CategoricalCrossEntropy(y_pred, y_true):\n",
    "    return y_pred - y_true\n",
    " \n",
    "\n",
    "def L2_loss(y_pred, y_true):\n",
    "    return np.mean(np.sum((y_pred - y_true)**2, axis=1), axis=0)\n",
    "\n",
    "\n",
    "def D_L2_loss(y_pred, y_true):\n",
    "    return 2 * (y_pred - y_true) / y_pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class DenseLayer:\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs, activation, D_activation):\n",
    "        self.weights = np.random.randn(n_outputs, n_inputs) / np.sqrt(n_inputs * n_outputs)\n",
    "        self.bias = np.random.randn(n_outputs) / np.sqrt(n_outputs)\n",
    "        self.activation = activation\n",
    "        self.D_activation = D_activation\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.x = None\n",
    "        self.D_y = None\n",
    "        self.weights_error_lst = []\n",
    "        self.bias_error_lst = []\n",
    "        self.weights_error_rp = 0\n",
    "        self.bias_error_rp = 0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        z = np.dot(self.weights, x) + self.bias\n",
    "        self.D_y = self.D_activation(z)\n",
    "        y = self.activation(z)\n",
    "        return y\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        bias_error = np.dot(self.D_y.T, output_error)\n",
    "        self.bias_error_lst.append(bias_error)\n",
    "        weights_error = np.outer(bias_error, self.x)\n",
    "        self.weights_error_lst.append(weights_error)\n",
    "        input_error = np.dot(self.weights.T, bias_error)\n",
    "        return input_error\n",
    "    \n",
    "    def update(self, learning_rate, adaption_rate):\n",
    "        weights_error = np.mean(np.array(self.weights_error_lst), axis=0)\n",
    "        self.weights_error_rp = np.clip(adaption_rate * self.weights_error_rp + (1 - adaption_rate) * weights_error**2, 1e-9, np.inf)\n",
    "        self.weights -= learning_rate / (np.sqrt(self.weights_error_rp)) * weights_error\n",
    "        self.weights_error_lst = []\n",
    "        bias_error = np.mean(np.array(self.bias_error_lst), axis=0)\n",
    "        self.bias_error_rp = np.clip(adaption_rate * self.bias_error_rp + (1 - adaption_rate) * bias_error**2, 1e-9, np.inf)\n",
    "        self.bias -= learning_rate / (np.sqrt(self.bias_error_rp)) * bias_error\n",
    "        self.bias_error_lst = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.weights = np.random.randn(self.n_outputs, self.n_inputs) / np.sqrt(self.n_inputs * self.n_outputs)\n",
    "        self.bias = np.random.randn(self.n_outputs) / np.sqrt(self.n_outputs)\n",
    "        self.x = None\n",
    "        self.D_y = None\n",
    "        self.weights_error_lst = []\n",
    "        self.bias_error_lst = []\n",
    "        self.weights_error_rp = 0\n",
    "        self.bias_error_rp = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "\n",
    "class ConvolutionalLayer:\n",
    "\n",
    "    def __init__(self, n_filter, filter_shape, padding='valid'):\n",
    "        self.filter = np.random.rand(n_filter, *filter_shape)\n",
    "        self.padding = padding\n",
    "        self.x = None\n",
    "        self.D_y = None\n",
    "        self.filter_error_lst = []\n",
    "        self.filter_error_rp = 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def activation(x):\n",
    "        return np.maximum(x, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def D_activation(x):\n",
    "        return (x >= 0).astype(float)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        depth_lst = []\n",
    "        for depth_filter in self.filter:\n",
    "            depth_lst.append(convolve2d(x, depth_filter, mode=self.padding, boundary='fill', fillvalue=0))\n",
    "        z = np.array(depth_lst)\n",
    "        depth_lst = []\n",
    "        for depth_z in z:\n",
    "            depth_lst.append(self.D_activation(depth_z))\n",
    "        self.D_y = np.array(depth_lst)\n",
    "        y = self.activation(z)\n",
    "        return y\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        activation_error = self.D_y * output_error\n",
    "        depth_lst = []\n",
    "        for depth_activation_error in activation_error:\n",
    "            depth_lst.append(convolve2d(self.x, depth_activation_error, mode=self.padding, boundary='fill', fillvalue=0))\n",
    "        filter_error = np.array(depth_lst)\n",
    "        self.filter_error_lst.append(filter_error)\n",
    "        depth_lst = []\n",
    "        for depth_activation_error, depth_filter in zip(activation_error, self.filter):\n",
    "            depth_lst.append(convolve2d(np.flipud(np.fliplr(depth_filter)), depth_activation_error, mode='full', boundary='fill', fillvalue=0))\n",
    "        input_error = np.array(depth_lst)\n",
    "        return input_error\n",
    "    \n",
    "    def update(self, learning_rate, adaption_rate):\n",
    "        filter_error = np.mean(np.array(self.filter_error_lst), axis=0)\n",
    "        self.filter_error_rp = np.clip(adaption_rate * self.filter_error_rp + (1 - adaption_rate) * filter_error**2, 1e-9, np.inf)\n",
    "        self.filter -= learning_rate / (np.sqrt(self.filter_error_rp)) * filter_error\n",
    "        self.filter_error_lst = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.x = None\n",
    "        self.D_y = None\n",
    "        self.filter_error_lst = []\n",
    "        self.filter_error_rp = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenLayer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.flatten_shape = None\n",
    "\n",
    "    def forward(self, x):\n",
    "       self.flatten_shape = x.shape\n",
    "       y = x.flatten()\n",
    "       return y\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        input_error = output_error.reshape(self.flatten_shape)\n",
    "        return input_error\n",
    "    \n",
    "    def update(self, learning_rate, adaption_rate):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        self.flatten_shape = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class MaxPoolingLayer:\n",
    "\n",
    "    def __init__(self, stride=2):\n",
    "        self.stride = stride\n",
    "        self.max_pool = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        depth_y_lst = []\n",
    "        depth_max_pool_lst = []\n",
    "        for depth_x in x:\n",
    "            dim_1_range = range(0, self.stride * (int(depth_x.shape[0] / self.stride)), self.stride)\n",
    "            dim_2_range = range(0, self.stride * (int(depth_x.shape[1] / self.stride)), self.stride)\n",
    "            depth_max_pool = np.zeros_like(depth_x)\n",
    "            depth_y = np.zeros([len(dim_1_range), len(dim_2_range)])\n",
    "            for index_y_1, index_pool_1 in enumerate(dim_1_range):\n",
    "                for index_y_2, index_pool_2 in enumerate(dim_2_range):\n",
    "                    index_1_pool_slice = slice(index_pool_1, index_pool_1+self.stride)\n",
    "                    index_2_pool_slice = slice(index_pool_2, index_pool_2+self.stride)\n",
    "                    depth_x_pool = depth_x[index_1_pool_slice, index_2_pool_slice]\n",
    "                    index_1_y_max, index_2_y_max = np.unravel_index(np.argmax(depth_x_pool), (self.stride, self.stride))\n",
    "                    depth_y[index_y_1, index_y_2] = depth_x_pool[index_1_y_max, index_2_y_max]\n",
    "                    depth_max_pool[index_1_pool_slice, index_2_pool_slice][index_1_y_max, index_2_y_max] = 1\n",
    "            depth_y_lst.append(depth_y)\n",
    "            depth_max_pool_lst.append(depth_max_pool)\n",
    "        y = np.array(depth_y_lst)\n",
    "        self.max_pool = np.array(depth_max_pool_lst)\n",
    "        return y\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        input_error = self.max_pool\n",
    "        return input_error\n",
    "    \n",
    "    def update(self, learning_rate, adaption_rate):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        self.max_pool = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class ClassificationNetwork:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.layer_lst = []\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred_lst = []\n",
    "        for x in X:\n",
    "            y_pred_lst.append(self._forward(x))\n",
    "        y_pred = np.array(y_pred_lst)\n",
    "        return y_pred\n",
    "\n",
    "    def _forward(self, x):\n",
    "        y_pred = x\n",
    "        for layer in self.layer_lst:\n",
    "            y_pred = layer.forward(y_pred)\n",
    "        return y_pred\n",
    "    \n",
    "    def _backward(self, y):\n",
    "        y_pred = y\n",
    "        for layer in reversed(self.layer_lst):\n",
    "            y_pred = layer.backward(y_pred)\n",
    "        return y_pred\n",
    "\n",
    "    def _update(self, learning_rate, adaption_rate):\n",
    "        for layer in self.layer_lst:\n",
    "            layer.update(learning_rate, adaption_rate)\n",
    "\n",
    "    def _reset(self):\n",
    "        for layer in self.layer_lst:\n",
    "            layer.reset()\n",
    "\n",
    "    def fit(self, X_train, y_train, learning_rate=1e-1, adaption_rate=0.9, N_epochs=1000, N_batch=100):\n",
    "        self._reset()\n",
    "        for index_epoch in range(N_epochs):\n",
    "            batch_indices = np.random.randint(0, X_train.shape[0] - 1, [N_batch])\n",
    "            X_batch = X_train[batch_indices]\n",
    "            y_batch = y_train[batch_indices]\n",
    "            y_pred = np.full_like(y_batch, fill_value=np.nan)\n",
    "            for index_batch, (x, y) in enumerate(zip(X_batch, y_batch)):\n",
    "                y_pred[index_batch, :] = self._forward(x)\n",
    "                self._backward(D_CategoricalCrossEntropy(y_pred[index_batch, :], y))\n",
    "            self._update(learning_rate, adaption_rate)\n",
    "            loss = CategoricalCrossEntropy(y_pred, y_batch)\n",
    "            print(\"Epoch {}/{} Loss: {}\".format(int(index_epoch + 1), N_epochs, loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_epochs = 100\n",
    "N_batch = 100\n",
    "N_convolutional_layer_kernel_shape = (3, 3)\n",
    "N_convolutional_layer_depth = 32\n",
    "N_dense_relu_inputs = 5408\n",
    "# N_dense_relu_inputs = 21632\n",
    "N_dense_relu_outputs = 64\n",
    "N_dense_softmax_inputs = N_dense_relu_outputs\n",
    "N_dense_softmax_outputs = train_labels.shape[1]\n",
    "learning_rate = 1e-4\n",
    "adaption_rate = 0.5\n",
    "\n",
    "network = ClassificationNetwork()\n",
    "network.layer_lst.append(ConvolutionalLayer(N_convolutional_layer_depth, N_convolutional_layer_kernel_shape))\n",
    "network.layer_lst.append(MaxPoolingLayer())\n",
    "network.layer_lst.append(FlattenLayer())\n",
    "network.layer_lst.append(DenseLayer(N_dense_relu_inputs, N_dense_relu_outputs, ReLU, D_ReLU))\n",
    "network.layer_lst.append(DenseLayer(N_dense_softmax_inputs, N_dense_softmax_outputs, SoftMax, D_SoftMax))\n",
    "network.fit(train_images, train_labels, learning_rate, adaption_rate, N_epochs, N_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = network.predict(test_images)\n",
    "accuracy = np.sum(np.argmax(predicted_labels, axis=1) == np.argmax(test_labels, axis=1)) / predicted_labels.shape[0] * 100\n",
    "print(\"Accuracy: {:.1f}%\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
