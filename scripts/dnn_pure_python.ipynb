{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500 Loss: 4.748495578765869\n",
      "Epoch 2/500 Loss: 4.861515045166016\n",
      "Epoch 3/500 Loss: 4.797658443450928\n",
      "Epoch 4/500 Loss: 4.752599239349365\n",
      "Epoch 5/500 Loss: 4.68756628036499\n",
      "Epoch 6/500 Loss: 4.7170305252075195\n",
      "Epoch 7/500 Loss: 4.7786478996276855\n",
      "Epoch 8/500 Loss: 4.741768836975098\n",
      "Epoch 9/500 Loss: 4.740433216094971\n",
      "Epoch 10/500 Loss: 4.701882839202881\n",
      "Epoch 11/500 Loss: 4.652900218963623\n",
      "Epoch 12/500 Loss: 4.663219451904297\n",
      "Epoch 13/500 Loss: 4.629430294036865\n",
      "Epoch 14/500 Loss: 4.675034046173096\n",
      "Epoch 15/500 Loss: 4.615380764007568\n",
      "Epoch 16/500 Loss: 4.614375114440918\n",
      "Epoch 17/500 Loss: 4.617863655090332\n",
      "Epoch 18/500 Loss: 4.549555778503418\n",
      "Epoch 19/500 Loss: 4.483547210693359\n",
      "Epoch 20/500 Loss: 4.527151584625244\n",
      "Epoch 21/500 Loss: 4.393606185913086\n",
      "Epoch 22/500 Loss: 4.721968173980713\n",
      "Epoch 23/500 Loss: 4.490753173828125\n",
      "Epoch 24/500 Loss: 4.353170871734619\n",
      "Epoch 25/500 Loss: 4.179644584655762\n",
      "Epoch 26/500 Loss: 4.254596710205078\n",
      "Epoch 27/500 Loss: 4.249398231506348\n",
      "Epoch 28/500 Loss: 4.202326774597168\n",
      "Epoch 29/500 Loss: 4.118574619293213\n",
      "Epoch 30/500 Loss: 4.393912315368652\n",
      "Epoch 31/500 Loss: 3.782529354095459\n",
      "Epoch 32/500 Loss: 4.3884663581848145\n",
      "Epoch 33/500 Loss: 4.466972351074219\n",
      "Epoch 34/500 Loss: 4.215557098388672\n",
      "Epoch 35/500 Loss: 3.984376907348633\n",
      "Epoch 36/500 Loss: 4.154173851013184\n",
      "Epoch 37/500 Loss: 4.155956268310547\n",
      "Epoch 38/500 Loss: 4.417973518371582\n",
      "Epoch 39/500 Loss: 3.991354465484619\n",
      "Epoch 40/500 Loss: 3.94643497467041\n",
      "Epoch 41/500 Loss: 3.7780404090881348\n",
      "Epoch 42/500 Loss: 4.092311859130859\n",
      "Epoch 43/500 Loss: 4.688786506652832\n",
      "Epoch 44/500 Loss: 4.215817451477051\n",
      "Epoch 45/500 Loss: 4.168299674987793\n",
      "Epoch 46/500 Loss: 4.021932125091553\n",
      "Epoch 47/500 Loss: 4.193103790283203\n",
      "Epoch 48/500 Loss: 3.902782917022705\n",
      "Epoch 49/500 Loss: 3.623929023742676\n",
      "Epoch 50/500 Loss: 3.731066942214966\n",
      "Epoch 51/500 Loss: 3.8150031566619873\n",
      "Epoch 52/500 Loss: 3.571485996246338\n",
      "Epoch 53/500 Loss: 3.6557867527008057\n",
      "Epoch 54/500 Loss: 3.6850502490997314\n",
      "Epoch 55/500 Loss: 3.788942337036133\n",
      "Epoch 56/500 Loss: 3.521231174468994\n",
      "Epoch 57/500 Loss: 3.454835891723633\n",
      "Epoch 58/500 Loss: 3.902308464050293\n",
      "Epoch 59/500 Loss: 3.281953811645508\n",
      "Epoch 60/500 Loss: 3.771693706512451\n",
      "Epoch 61/500 Loss: 3.5798118114471436\n",
      "Epoch 62/500 Loss: 3.4388725757598877\n",
      "Epoch 63/500 Loss: 3.5155560970306396\n",
      "Epoch 64/500 Loss: 3.318817377090454\n",
      "Epoch 65/500 Loss: 3.8915975093841553\n",
      "Epoch 66/500 Loss: 3.55674409866333\n",
      "Epoch 67/500 Loss: 4.376710414886475\n",
      "Epoch 68/500 Loss: 3.2496960163116455\n",
      "Epoch 69/500 Loss: 3.6127278804779053\n",
      "Epoch 70/500 Loss: 3.143754482269287\n",
      "Epoch 71/500 Loss: 3.584055185317993\n",
      "Epoch 72/500 Loss: 3.5647287368774414\n",
      "Epoch 73/500 Loss: 3.49027156829834\n",
      "Epoch 74/500 Loss: 3.3791534900665283\n",
      "Epoch 75/500 Loss: 3.152651071548462\n",
      "Epoch 76/500 Loss: 3.600442409515381\n",
      "Epoch 77/500 Loss: 3.557978630065918\n",
      "Epoch 78/500 Loss: 3.5457923412323\n",
      "Epoch 79/500 Loss: 3.147233486175537\n",
      "Epoch 80/500 Loss: 3.352910041809082\n",
      "Epoch 81/500 Loss: 3.238076686859131\n",
      "Epoch 82/500 Loss: 3.0478546619415283\n",
      "Epoch 83/500 Loss: 3.531625747680664\n",
      "Epoch 84/500 Loss: 3.2170116901397705\n",
      "Epoch 85/500 Loss: 2.857640266418457\n",
      "Epoch 86/500 Loss: 3.3281216621398926\n",
      "Epoch 87/500 Loss: 2.93257474899292\n",
      "Epoch 88/500 Loss: 3.194704055786133\n",
      "Epoch 89/500 Loss: 2.7521514892578125\n",
      "Epoch 90/500 Loss: 3.0045855045318604\n",
      "Epoch 91/500 Loss: 3.4800057411193848\n",
      "Epoch 92/500 Loss: 3.1655161380767822\n",
      "Epoch 93/500 Loss: 2.67474365234375\n",
      "Epoch 94/500 Loss: 2.7988176345825195\n",
      "Epoch 95/500 Loss: 3.192577600479126\n",
      "Epoch 96/500 Loss: 2.6516551971435547\n",
      "Epoch 97/500 Loss: 2.832932233810425\n",
      "Epoch 98/500 Loss: 3.1038613319396973\n",
      "Epoch 99/500 Loss: 2.7429075241088867\n",
      "Epoch 100/500 Loss: 2.9854955673217773\n",
      "Epoch 101/500 Loss: 2.8362882137298584\n",
      "Epoch 102/500 Loss: 2.862151861190796\n",
      "Epoch 103/500 Loss: 2.855386972427368\n",
      "Epoch 104/500 Loss: 2.6749112606048584\n",
      "Epoch 105/500 Loss: 2.709500789642334\n",
      "Epoch 106/500 Loss: 2.6209399700164795\n",
      "Epoch 107/500 Loss: 2.5668160915374756\n",
      "Epoch 108/500 Loss: 2.3188693523406982\n",
      "Epoch 109/500 Loss: 2.6456916332244873\n",
      "Epoch 110/500 Loss: 2.6496503353118896\n",
      "Epoch 111/500 Loss: 2.5352537631988525\n",
      "Epoch 112/500 Loss: 2.749633550643921\n",
      "Epoch 113/500 Loss: 2.5223793983459473\n",
      "Epoch 114/500 Loss: 2.5026497840881348\n",
      "Epoch 115/500 Loss: 2.3876376152038574\n",
      "Epoch 116/500 Loss: 2.1825368404388428\n",
      "Epoch 117/500 Loss: 2.269502639770508\n",
      "Epoch 118/500 Loss: 2.6271462440490723\n",
      "Epoch 119/500 Loss: 2.418184757232666\n",
      "Epoch 120/500 Loss: 2.2342209815979004\n",
      "Epoch 121/500 Loss: 2.499122381210327\n",
      "Epoch 122/500 Loss: 2.3926961421966553\n",
      "Epoch 123/500 Loss: 1.9331070184707642\n",
      "Epoch 124/500 Loss: 2.5364632606506348\n",
      "Epoch 125/500 Loss: 2.0340256690979004\n",
      "Epoch 126/500 Loss: 2.071479558944702\n",
      "Epoch 127/500 Loss: 2.142822027206421\n",
      "Epoch 128/500 Loss: 2.1005752086639404\n",
      "Epoch 129/500 Loss: 1.9280723333358765\n",
      "Epoch 130/500 Loss: 2.0987982749938965\n",
      "Epoch 131/500 Loss: 1.8384146690368652\n",
      "Epoch 132/500 Loss: 2.384347438812256\n",
      "Epoch 133/500 Loss: 2.2208662033081055\n",
      "Epoch 134/500 Loss: 2.0714704990386963\n",
      "Epoch 135/500 Loss: 1.9123802185058594\n",
      "Epoch 136/500 Loss: 1.76088547706604\n",
      "Epoch 137/500 Loss: 1.9271408319473267\n",
      "Epoch 138/500 Loss: 2.0801494121551514\n",
      "Epoch 139/500 Loss: 2.110093116760254\n",
      "Epoch 140/500 Loss: 2.0417041778564453\n",
      "Epoch 141/500 Loss: 2.717843532562256\n",
      "Epoch 142/500 Loss: 1.863510251045227\n",
      "Epoch 143/500 Loss: 1.7319477796554565\n",
      "Epoch 144/500 Loss: 2.066955804824829\n",
      "Epoch 145/500 Loss: 2.6403939723968506\n",
      "Epoch 146/500 Loss: 2.0059492588043213\n",
      "Epoch 147/500 Loss: 1.6348398923873901\n",
      "Epoch 148/500 Loss: 1.6452313661575317\n",
      "Epoch 149/500 Loss: 1.478533148765564\n",
      "Epoch 150/500 Loss: 2.082075357437134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3f/d8gl3k3j0bn3wk_cqjvfm31w0000gn/T/ipykernel_80803/3777856352.py:24: RuntimeWarning: divide by zero encountered in log2\n",
      "  return np.mean(-1 * np.sum(y_true * np.log2(tmp) + (1 - y_true) * np.log2(1 - tmp), axis=1), axis=0)\n",
      "/var/folders/3f/d8gl3k3j0bn3wk_cqjvfm31w0000gn/T/ipykernel_80803/3777856352.py:24: RuntimeWarning: invalid value encountered in multiply\n",
      "  return np.mean(-1 * np.sum(y_true * np.log2(tmp) + (1 - y_true) * np.log2(1 - tmp), axis=1), axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/500 Loss: nan\n",
      "Epoch 152/500 Loss: 1.848251461982727\n",
      "Epoch 153/500 Loss: 1.880061149597168\n",
      "Epoch 154/500 Loss: 2.217270612716675\n",
      "Epoch 155/500 Loss: 1.5706329345703125\n",
      "Epoch 156/500 Loss: 1.9271140098571777\n",
      "Epoch 157/500 Loss: 1.8588247299194336\n",
      "Epoch 158/500 Loss: 1.5861353874206543\n",
      "Epoch 159/500 Loss: 1.3591517210006714\n",
      "Epoch 160/500 Loss: 1.7333853244781494\n",
      "Epoch 161/500 Loss: 1.44783616065979\n",
      "Epoch 162/500 Loss: 1.832228422164917\n",
      "Epoch 163/500 Loss: 1.5498133897781372\n",
      "Epoch 164/500 Loss: 1.8467499017715454\n",
      "Epoch 165/500 Loss: 1.7699203491210938\n",
      "Epoch 166/500 Loss: 1.3292169570922852\n",
      "Epoch 167/500 Loss: 1.5200821161270142\n",
      "Epoch 168/500 Loss: 1.9574588537216187\n",
      "Epoch 169/500 Loss: 1.4956345558166504\n",
      "Epoch 170/500 Loss: 1.3851425647735596\n",
      "Epoch 171/500 Loss: 1.1459887027740479\n",
      "Epoch 172/500 Loss: nan\n",
      "Epoch 173/500 Loss: 1.8185843229293823\n",
      "Epoch 174/500 Loss: 2.2549004554748535\n",
      "Epoch 175/500 Loss: 1.6822094917297363\n",
      "Epoch 176/500 Loss: 1.6098151206970215\n",
      "Epoch 177/500 Loss: 1.7381914854049683\n",
      "Epoch 178/500 Loss: 1.5366908311843872\n",
      "Epoch 179/500 Loss: 1.4144041538238525\n",
      "Epoch 180/500 Loss: 1.1991904973983765\n",
      "Epoch 181/500 Loss: 1.1620687246322632\n",
      "Epoch 182/500 Loss: 1.6681898832321167\n",
      "Epoch 183/500 Loss: 1.158599853515625\n",
      "Epoch 184/500 Loss: 1.022627353668213\n",
      "Epoch 185/500 Loss: 1.9514906406402588\n",
      "Epoch 186/500 Loss: 1.4180908203125\n",
      "Epoch 187/500 Loss: 1.6301705837249756\n",
      "Epoch 188/500 Loss: 2.117579460144043\n",
      "Epoch 189/500 Loss: 1.8458813428878784\n",
      "Epoch 190/500 Loss: 0.966452956199646\n",
      "Epoch 191/500 Loss: 1.7399945259094238\n",
      "Epoch 192/500 Loss: 0.9630058407783508\n",
      "Epoch 193/500 Loss: 1.7761174440383911\n",
      "Epoch 194/500 Loss: 1.5011918544769287\n",
      "Epoch 195/500 Loss: 0.9911341667175293\n",
      "Epoch 196/500 Loss: 1.521712303161621\n",
      "Epoch 197/500 Loss: 1.299992322921753\n",
      "Epoch 198/500 Loss: 1.4566137790679932\n",
      "Epoch 199/500 Loss: 1.3216948509216309\n",
      "Epoch 200/500 Loss: 1.3698244094848633\n",
      "Epoch 201/500 Loss: 1.5998965501785278\n",
      "Epoch 202/500 Loss: 1.046573281288147\n",
      "Epoch 203/500 Loss: 1.179226279258728\n",
      "Epoch 204/500 Loss: 1.6707298755645752\n",
      "Epoch 205/500 Loss: 1.2234658002853394\n",
      "Epoch 206/500 Loss: 0.9186842441558838\n",
      "Epoch 207/500 Loss: 1.3515032529830933\n",
      "Epoch 208/500 Loss: 1.0308104753494263\n",
      "Epoch 209/500 Loss: 1.4068799018859863\n",
      "Epoch 210/500 Loss: 1.8614630699157715\n",
      "Epoch 211/500 Loss: 1.5276530981063843\n",
      "Epoch 212/500 Loss: 1.0594401359558105\n",
      "Epoch 213/500 Loss: 0.9813348650932312\n",
      "Epoch 214/500 Loss: 0.943626344203949\n",
      "Epoch 215/500 Loss: 0.9765188694000244\n",
      "Epoch 216/500 Loss: 1.122045874595642\n",
      "Epoch 217/500 Loss: 1.2758959531784058\n",
      "Epoch 218/500 Loss: 1.6484102010726929\n",
      "Epoch 219/500 Loss: 1.4906927347183228\n",
      "Epoch 220/500 Loss: 1.1182470321655273\n",
      "Epoch 221/500 Loss: 1.0812938213348389\n",
      "Epoch 222/500 Loss: 1.214989423751831\n",
      "Epoch 223/500 Loss: 1.1735228300094604\n",
      "Epoch 224/500 Loss: 1.5732269287109375\n",
      "Epoch 225/500 Loss: 1.101609230041504\n",
      "Epoch 226/500 Loss: 0.906325101852417\n",
      "Epoch 227/500 Loss: 1.1397218704223633\n",
      "Epoch 228/500 Loss: 0.7858512997627258\n",
      "Epoch 229/500 Loss: 1.4006479978561401\n",
      "Epoch 230/500 Loss: 0.7494359016418457\n",
      "Epoch 231/500 Loss: 1.221943736076355\n",
      "Epoch 232/500 Loss: 1.3308826684951782\n",
      "Epoch 233/500 Loss: 0.9560322761535645\n",
      "Epoch 234/500 Loss: 1.4201017618179321\n",
      "Epoch 235/500 Loss: 1.2914175987243652\n",
      "Epoch 236/500 Loss: 1.2352516651153564\n",
      "Epoch 237/500 Loss: 0.735076367855072\n",
      "Epoch 238/500 Loss: 0.9581889510154724\n",
      "Epoch 239/500 Loss: 0.8441748023033142\n",
      "Epoch 240/500 Loss: 0.6412738561630249\n",
      "Epoch 241/500 Loss: 1.1114847660064697\n",
      "Epoch 242/500 Loss: 0.9397729635238647\n",
      "Epoch 243/500 Loss: 1.1391921043395996\n",
      "Epoch 244/500 Loss: 1.2992936372756958\n",
      "Epoch 245/500 Loss: 1.4007236957550049\n",
      "Epoch 246/500 Loss: 0.6197496056556702\n",
      "Epoch 247/500 Loss: 0.6670711636543274\n",
      "Epoch 248/500 Loss: 0.8071635365486145\n",
      "Epoch 249/500 Loss: 0.9540635943412781\n",
      "Epoch 250/500 Loss: 1.0104271173477173\n",
      "Epoch 251/500 Loss: 1.4132460355758667\n",
      "Epoch 252/500 Loss: 0.7723627686500549\n",
      "Epoch 253/500 Loss: 0.502302348613739\n",
      "Epoch 254/500 Loss: 1.2901002168655396\n",
      "Epoch 255/500 Loss: 1.7220067977905273\n",
      "Epoch 256/500 Loss: 1.0986706018447876\n",
      "Epoch 257/500 Loss: 1.0778398513793945\n",
      "Epoch 258/500 Loss: 1.3242428302764893\n",
      "Epoch 259/500 Loss: 1.764617919921875\n",
      "Epoch 260/500 Loss: 1.1055258512496948\n",
      "Epoch 261/500 Loss: 0.8852601647377014\n",
      "Epoch 262/500 Loss: 0.7195026278495789\n",
      "Epoch 263/500 Loss: 0.7079049944877625\n",
      "Epoch 264/500 Loss: 1.2211354970932007\n",
      "Epoch 265/500 Loss: 1.5379328727722168\n",
      "Epoch 266/500 Loss: 1.1477102041244507\n",
      "Epoch 267/500 Loss: 1.0571037530899048\n",
      "Epoch 268/500 Loss: 1.2549647092819214\n",
      "Epoch 269/500 Loss: 1.430169701576233\n",
      "Epoch 270/500 Loss: 0.7845766544342041\n",
      "Epoch 271/500 Loss: 1.0569493770599365\n",
      "Epoch 272/500 Loss: 1.600751519203186\n",
      "Epoch 273/500 Loss: 0.7810677886009216\n",
      "Epoch 274/500 Loss: 0.9864007830619812\n",
      "Epoch 275/500 Loss: 1.6492592096328735\n",
      "Epoch 276/500 Loss: 0.6851611137390137\n",
      "Epoch 277/500 Loss: 0.7889576554298401\n",
      "Epoch 278/500 Loss: 0.721828281879425\n",
      "Epoch 279/500 Loss: 1.4504368305206299\n",
      "Epoch 280/500 Loss: 1.0304980278015137\n",
      "Epoch 281/500 Loss: 1.0116419792175293\n",
      "Epoch 282/500 Loss: 0.3940993845462799\n",
      "Epoch 283/500 Loss: 0.46149104833602905\n",
      "Epoch 284/500 Loss: 0.8814321756362915\n",
      "Epoch 285/500 Loss: 1.0491183996200562\n",
      "Epoch 286/500 Loss: 0.7708340287208557\n",
      "Epoch 287/500 Loss: 1.3658270835876465\n",
      "Epoch 288/500 Loss: 1.265896201133728\n",
      "Epoch 289/500 Loss: 1.2414259910583496\n",
      "Epoch 290/500 Loss: 1.0580521821975708\n",
      "Epoch 291/500 Loss: 1.6670738458633423\n",
      "Epoch 292/500 Loss: 1.07230806350708\n",
      "Epoch 293/500 Loss: 1.6960804462432861\n",
      "Epoch 294/500 Loss: 0.7788219451904297\n",
      "Epoch 295/500 Loss: 0.7511953115463257\n",
      "Epoch 296/500 Loss: 0.8611507415771484\n",
      "Epoch 297/500 Loss: 0.9411419630050659\n",
      "Epoch 298/500 Loss: 0.45822030305862427\n",
      "Epoch 299/500 Loss: 0.6017935872077942\n",
      "Epoch 300/500 Loss: 0.814609706401825\n",
      "Epoch 301/500 Loss: 0.46831268072128296\n",
      "Epoch 302/500 Loss: 0.9404800534248352\n",
      "Epoch 303/500 Loss: 0.6707122325897217\n",
      "Epoch 304/500 Loss: 0.821424126625061\n",
      "Epoch 305/500 Loss: 0.7776168584823608\n",
      "Epoch 306/500 Loss: 0.8985791206359863\n",
      "Epoch 307/500 Loss: 1.3439449071884155\n",
      "Epoch 308/500 Loss: 1.5210973024368286\n",
      "Epoch 309/500 Loss: 1.1115517616271973\n",
      "Epoch 310/500 Loss: 0.4081723093986511\n",
      "Epoch 311/500 Loss: 0.5917728543281555\n",
      "Epoch 312/500 Loss: 0.36798587441444397\n",
      "Epoch 313/500 Loss: 1.056586742401123\n",
      "Epoch 314/500 Loss: 0.37026992440223694\n",
      "Epoch 315/500 Loss: 1.074476957321167\n",
      "Epoch 316/500 Loss: 0.8325017690658569\n",
      "Epoch 317/500 Loss: 0.700161337852478\n",
      "Epoch 318/500 Loss: 1.2463833093643188\n",
      "Epoch 319/500 Loss: 0.8104509711265564\n",
      "Epoch 320/500 Loss: 0.9305732250213623\n",
      "Epoch 321/500 Loss: 0.9989882707595825\n",
      "Epoch 322/500 Loss: 0.9753658175468445\n",
      "Epoch 323/500 Loss: 0.5871371626853943\n",
      "Epoch 324/500 Loss: 0.570814847946167\n",
      "Epoch 325/500 Loss: 0.8654619455337524\n",
      "Epoch 326/500 Loss: 1.0591566562652588\n",
      "Epoch 327/500 Loss: 1.0952415466308594\n",
      "Epoch 328/500 Loss: 0.8719207644462585\n",
      "Epoch 329/500 Loss: 0.5587100386619568\n",
      "Epoch 330/500 Loss: 0.6139081120491028\n",
      "Epoch 331/500 Loss: 1.2193259000778198\n",
      "Epoch 332/500 Loss: 0.9051758050918579\n",
      "Epoch 333/500 Loss: 1.3686285018920898\n",
      "Epoch 334/500 Loss: 1.170807957649231\n",
      "Epoch 335/500 Loss: 0.5241472721099854\n",
      "Epoch 336/500 Loss: 0.46049851179122925\n",
      "Epoch 337/500 Loss: 0.985639214515686\n",
      "Epoch 338/500 Loss: 0.5453788638114929\n",
      "Epoch 339/500 Loss: 0.5827493667602539\n",
      "Epoch 340/500 Loss: 0.9349654316902161\n",
      "Epoch 341/500 Loss: 0.6068050265312195\n",
      "Epoch 342/500 Loss: 0.6054937243461609\n",
      "Epoch 343/500 Loss: 0.9881472587585449\n",
      "Epoch 344/500 Loss: 0.9500295519828796\n",
      "Epoch 345/500 Loss: 0.925971269607544\n",
      "Epoch 346/500 Loss: 0.7548785209655762\n",
      "Epoch 347/500 Loss: 0.6636157035827637\n",
      "Epoch 348/500 Loss: 0.8667876720428467\n",
      "Epoch 349/500 Loss: 0.9882623553276062\n",
      "Epoch 350/500 Loss: 1.1810212135314941\n",
      "Epoch 351/500 Loss: 0.915047824382782\n",
      "Epoch 352/500 Loss: 0.8394380211830139\n",
      "Epoch 353/500 Loss: 0.9901362657546997\n",
      "Epoch 354/500 Loss: 0.7776807546615601\n",
      "Epoch 355/500 Loss: 0.6288947463035583\n",
      "Epoch 356/500 Loss: 0.6900057196617126\n",
      "Epoch 357/500 Loss: 0.9762077331542969\n",
      "Epoch 358/500 Loss: 0.3239339590072632\n",
      "Epoch 359/500 Loss: 1.0858341455459595\n",
      "Epoch 360/500 Loss: 1.263068675994873\n",
      "Epoch 361/500 Loss: 0.7114543318748474\n",
      "Epoch 362/500 Loss: 0.5039854645729065\n",
      "Epoch 363/500 Loss: 0.760699450969696\n",
      "Epoch 364/500 Loss: 1.0258318185806274\n",
      "Epoch 365/500 Loss: 1.0387985706329346\n",
      "Epoch 366/500 Loss: 0.9507842063903809\n",
      "Epoch 367/500 Loss: 0.7731082439422607\n",
      "Epoch 368/500 Loss: 0.8676213622093201\n",
      "Epoch 369/500 Loss: 1.1262788772583008\n",
      "Epoch 370/500 Loss: 1.0179715156555176\n",
      "Epoch 371/500 Loss: 0.7277847528457642\n",
      "Epoch 372/500 Loss: 0.2605742812156677\n",
      "Epoch 373/500 Loss: 1.1139072179794312\n",
      "Epoch 374/500 Loss: 0.9709769487380981\n",
      "Epoch 375/500 Loss: 0.48215073347091675\n",
      "Epoch 376/500 Loss: 1.9778693914413452\n",
      "Epoch 377/500 Loss: 0.6321654915809631\n",
      "Epoch 378/500 Loss: 0.9891453385353088\n",
      "Epoch 379/500 Loss: 0.5861255526542664\n",
      "Epoch 380/500 Loss: 1.085574984550476\n",
      "Epoch 381/500 Loss: 0.7368569374084473\n",
      "Epoch 382/500 Loss: 0.33237603306770325\n",
      "Epoch 383/500 Loss: 1.0687720775604248\n",
      "Epoch 384/500 Loss: 0.9190393686294556\n",
      "Epoch 385/500 Loss: 0.661804735660553\n",
      "Epoch 386/500 Loss: 0.5680612325668335\n",
      "Epoch 387/500 Loss: 0.7816950082778931\n",
      "Epoch 388/500 Loss: 0.8942641615867615\n",
      "Epoch 389/500 Loss: 0.9554355144500732\n",
      "Epoch 390/500 Loss: 0.33868512511253357\n",
      "Epoch 391/500 Loss: 0.6600448489189148\n",
      "Epoch 392/500 Loss: 0.7624220848083496\n",
      "Epoch 393/500 Loss: 0.3478250205516815\n",
      "Epoch 394/500 Loss: 0.8821003437042236\n",
      "Epoch 395/500 Loss: 0.872445821762085\n",
      "Epoch 396/500 Loss: 0.5320791602134705\n",
      "Epoch 397/500 Loss: 0.75910484790802\n",
      "Epoch 398/500 Loss: 1.038835883140564\n",
      "Epoch 399/500 Loss: 1.0289416313171387\n",
      "Epoch 400/500 Loss: 0.8585275411605835\n",
      "Epoch 401/500 Loss: 0.7390335202217102\n",
      "Epoch 402/500 Loss: 0.626594066619873\n",
      "Epoch 403/500 Loss: 0.4433914124965668\n",
      "Epoch 404/500 Loss: 0.7786485552787781\n",
      "Epoch 405/500 Loss: 0.2795954644680023\n",
      "Epoch 406/500 Loss: 0.6508256793022156\n",
      "Epoch 407/500 Loss: 0.7494213581085205\n",
      "Epoch 408/500 Loss: 0.5212295055389404\n",
      "Epoch 409/500 Loss: 0.6360287666320801\n",
      "Epoch 410/500 Loss: 0.384097695350647\n",
      "Epoch 411/500 Loss: 0.5273301601409912\n",
      "Epoch 412/500 Loss: 0.8305947780609131\n",
      "Epoch 413/500 Loss: 0.533037006855011\n",
      "Epoch 414/500 Loss: 0.5248991250991821\n",
      "Epoch 415/500 Loss: 0.712811291217804\n",
      "Epoch 416/500 Loss: 0.40954703092575073\n",
      "Epoch 417/500 Loss: 0.6201168298721313\n",
      "Epoch 418/500 Loss: 0.5494527220726013\n",
      "Epoch 419/500 Loss: 0.3223537504673004\n",
      "Epoch 420/500 Loss: 0.5725715160369873\n",
      "Epoch 421/500 Loss: 0.4504825174808502\n",
      "Epoch 422/500 Loss: 0.7011731863021851\n",
      "Epoch 423/500 Loss: 0.5726239085197449\n",
      "Epoch 424/500 Loss: 0.8055104613304138\n",
      "Epoch 425/500 Loss: 1.0237743854522705\n",
      "Epoch 426/500 Loss: 1.171288251876831\n",
      "Epoch 427/500 Loss: 0.9304782152175903\n",
      "Epoch 428/500 Loss: 1.2472819089889526\n",
      "Epoch 429/500 Loss: 0.7076780200004578\n",
      "Epoch 430/500 Loss: 0.8361131548881531\n",
      "Epoch 431/500 Loss: 0.6532991528511047\n",
      "Epoch 432/500 Loss: 0.568238377571106\n",
      "Epoch 433/500 Loss: 0.6436958909034729\n",
      "Epoch 434/500 Loss: 0.8866323828697205\n",
      "Epoch 435/500 Loss: 1.2512601613998413\n",
      "Epoch 436/500 Loss: 0.8047031164169312\n",
      "Epoch 437/500 Loss: 0.5968809723854065\n",
      "Epoch 438/500 Loss: 0.7735458612442017\n",
      "Epoch 439/500 Loss: 0.37863120436668396\n",
      "Epoch 440/500 Loss: 0.4178595244884491\n",
      "Epoch 441/500 Loss: 0.6049087643623352\n",
      "Epoch 442/500 Loss: 0.7759799957275391\n",
      "Epoch 443/500 Loss: 0.48484596610069275\n",
      "Epoch 444/500 Loss: 0.5392013192176819\n",
      "Epoch 445/500 Loss: 0.8330966830253601\n",
      "Epoch 446/500 Loss: 1.0225147008895874\n",
      "Epoch 447/500 Loss: 0.762846052646637\n",
      "Epoch 448/500 Loss: 0.9433504343032837\n",
      "Epoch 449/500 Loss: 0.5736316442489624\n",
      "Epoch 450/500 Loss: 0.5368587970733643\n",
      "Epoch 451/500 Loss: 0.8278223276138306\n",
      "Epoch 452/500 Loss: 0.7859355807304382\n",
      "Epoch 453/500 Loss: 0.5248647332191467\n",
      "Epoch 454/500 Loss: 0.5644175410270691\n",
      "Epoch 455/500 Loss: 0.8518706560134888\n",
      "Epoch 456/500 Loss: 0.24149395525455475\n",
      "Epoch 457/500 Loss: 0.7097504138946533\n",
      "Epoch 458/500 Loss: 0.4605167806148529\n",
      "Epoch 459/500 Loss: 0.4179997146129608\n",
      "Epoch 460/500 Loss: 0.5263805985450745\n",
      "Epoch 461/500 Loss: 0.9207032918930054\n",
      "Epoch 462/500 Loss: 0.32604190707206726\n",
      "Epoch 463/500 Loss: 1.3147947788238525\n",
      "Epoch 464/500 Loss: 0.5214584469795227\n",
      "Epoch 465/500 Loss: 0.537646472454071\n",
      "Epoch 466/500 Loss: 0.5579874515533447\n",
      "Epoch 467/500 Loss: 0.38972803950309753\n",
      "Epoch 468/500 Loss: 1.347225308418274\n",
      "Epoch 469/500 Loss: 0.5874533653259277\n",
      "Epoch 470/500 Loss: 0.42235079407691956\n",
      "Epoch 471/500 Loss: 0.4991941452026367\n",
      "Epoch 472/500 Loss: 0.33903181552886963\n",
      "Epoch 473/500 Loss: 1.0916694402694702\n",
      "Epoch 474/500 Loss: 0.6062790155410767\n",
      "Epoch 475/500 Loss: 0.4826289415359497\n",
      "Epoch 476/500 Loss: 1.2475193738937378\n",
      "Epoch 477/500 Loss: 0.8412151336669922\n",
      "Epoch 478/500 Loss: 0.9476249814033508\n",
      "Epoch 479/500 Loss: 0.5718023180961609\n",
      "Epoch 480/500 Loss: 0.3409934341907501\n",
      "Epoch 481/500 Loss: 0.32108187675476074\n",
      "Epoch 482/500 Loss: 0.347534716129303\n",
      "Epoch 483/500 Loss: 0.3689540922641754\n",
      "Epoch 484/500 Loss: 0.45353198051452637\n",
      "Epoch 485/500 Loss: 0.6746193170547485\n",
      "Epoch 486/500 Loss: 0.6508315205574036\n",
      "Epoch 487/500 Loss: 0.5839464664459229\n",
      "Epoch 488/500 Loss: 0.7245510816574097\n",
      "Epoch 489/500 Loss: 0.5069227814674377\n",
      "Epoch 490/500 Loss: 0.5933341383934021\n",
      "Epoch 491/500 Loss: 0.453159898519516\n",
      "Epoch 492/500 Loss: 0.1606227308511734\n",
      "Epoch 493/500 Loss: 0.41585004329681396\n",
      "Epoch 494/500 Loss: 0.4495950937271118\n",
      "Epoch 495/500 Loss: 0.4666811227798462\n",
      "Epoch 496/500 Loss: 0.3588741719722748\n",
      "Epoch 497/500 Loss: 0.4531428813934326\n",
      "Epoch 498/500 Loss: 0.8152837157249451\n",
      "Epoch 499/500 Loss: 0.8814996480941772\n",
      "Epoch 500/500 Loss: 0.6885308623313904\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "\n",
    "def D_ReLU(x):\n",
    "    return np.diag((x >= 0).astype(float))\n",
    "\n",
    "\n",
    "def SoftMax(x):\n",
    "    tmp = np.exp(x - np.max(x))\n",
    "    return tmp / np.sum(tmp)\n",
    "\n",
    "\n",
    "def D_SoftMax(x):\n",
    "    tmp = SoftMax(x)\n",
    "    return -1 * np.outer(tmp, tmp) + np.diag(tmp)\n",
    "\n",
    "\n",
    "def CategoricalCrossEntropy(y_pred, y_true):\n",
    "    tmp = np.clip(y_pred, 1e-8, 1-1e-8)\n",
    "    return np.mean(-1 * np.sum(y_true * np.log2(tmp) + (1 - y_true) * np.log2(1 - tmp), axis=1), axis=0)\n",
    "\n",
    "\n",
    "def D_CategoricalCrossEntropy(y_pred, y_true):\n",
    "    return y_pred - y_true\n",
    " \n",
    "\n",
    "def L2_loss(y_pred, y_true):\n",
    "    return np.mean(np.sum((y_pred - y_true)**2, axis=1), axis=0)\n",
    "\n",
    "\n",
    "def D_L2_loss(y_pred, y_true):\n",
    "    return 2 * (y_pred - y_true) / y_pred.shape[0]\n",
    "\n",
    "\n",
    "class DenseLayer:\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs, activation, D_activation):\n",
    "        self.weights = np.random.randn(n_outputs, n_inputs) / np.sqrt(n_inputs * n_outputs)\n",
    "        self.bias = np.random.randn(n_outputs) / np.sqrt(n_outputs)\n",
    "        self.activation = activation\n",
    "        self.D_activation = D_activation\n",
    "        self.x = None\n",
    "        self.D_y = None\n",
    "        self.weights_error_lst = []\n",
    "        self.bias_error_lst = []\n",
    "        self.weights_error_rp = 0\n",
    "        self.bias_error_rp = 0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        z = np.dot(self.weights, x) + self.bias\n",
    "        self.D_y = self.D_activation(z)\n",
    "        y = self.activation(z)\n",
    "        return y\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        bias_error = np.dot(self.D_y.T, output_error)\n",
    "        self.bias_error_lst.append(bias_error)\n",
    "        weights_error = np.outer(bias_error, self.x)\n",
    "        self.weights_error_lst.append(weights_error)\n",
    "        input_error = np.dot(self.weights.T, bias_error)\n",
    "        return input_error\n",
    "    \n",
    "    def update(self, learning_rate, adaption_rate):\n",
    "        weights_error = np.mean(np.array(self.weights_error_lst), axis=0)\n",
    "        self.weights_error_rp = np.clip(adaption_rate * self.weights_error_rp + (1 - adaption_rate) * weights_error**2, 1e-9, np.inf)\n",
    "        self.weights -= learning_rate / (np.sqrt(self.weights_error_rp)) * weights_error\n",
    "        self.weights_error_lst = []\n",
    "        bias_error = np.mean(np.array(self.bias_error_lst), axis=0)\n",
    "        self.bias_error_rp = np.clip(adaption_rate * self.bias_error_rp + (1 - adaption_rate) * bias_error**2, 1e-9, np.inf)\n",
    "        self.bias -= learning_rate / (np.sqrt(self.bias_error_rp)) * bias_error\n",
    "        self.bias_error_lst = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.x = None\n",
    "        self.D_y = None\n",
    "        self.weights_error_lst = []\n",
    "        self.bias_error_lst = []\n",
    "        self.weights_error_rp = 0\n",
    "        self.bias_error_rp = 0\n",
    "\n",
    "\n",
    "class Network:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.layer_lst = []\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred_lst = []\n",
    "        for x in X:\n",
    "            y_pred_lst.append(self._forward(x))\n",
    "        y_pred = np.array(y_pred_lst)\n",
    "        return y_pred\n",
    "\n",
    "    def _forward(self, x):\n",
    "        y_pred = x\n",
    "        for layer in self.layer_lst:\n",
    "            y_pred = layer.forward(y_pred)\n",
    "        return y_pred\n",
    "    \n",
    "    def _backward(self, y):\n",
    "        y_pred = y\n",
    "        for layer in reversed(self.layer_lst):\n",
    "            y_pred = layer.backward(y_pred)\n",
    "        return y_pred\n",
    "\n",
    "    def _update(self, learning_rate, adaption_rate):\n",
    "        for layer in self.layer_lst:\n",
    "            layer.update(learning_rate, adaption_rate)\n",
    "\n",
    "    def _reset(self):\n",
    "        for layer in self.layer_lst:\n",
    "            layer.reset()\n",
    "\n",
    "    def fit(self, X_train, y_train, learning_rate=1e-1, adaption_rate=0.9, N_epochs=1000, N_batch=100):\n",
    "        self._reset()\n",
    "        for index_epoch in range(N_epochs):\n",
    "            batch_indices = np.random.randint(0, X_train.shape[0] - 1, [N_batch])\n",
    "            X_batch = X_train[batch_indices]\n",
    "            y_batch = y_train[batch_indices]\n",
    "            y_pred = np.full_like(y_batch, fill_value=np.nan)\n",
    "            for index_batch, (x, y) in enumerate(zip(X_batch, y_batch)):\n",
    "                y_pred[index_batch, :] = self._forward(x)\n",
    "                self._backward(D_CategoricalCrossEntropy(y_pred[index_batch, :], y))\n",
    "            self._update(learning_rate, adaption_rate)\n",
    "            loss = CategoricalCrossEntropy(y_pred, y_batch)\n",
    "            print(\"Epoch {}/{} Loss: {}\".format(int(index_epoch + 1), N_epochs, loss))\n",
    "\n",
    "\n",
    "N_epochs = 500\n",
    "N_batch = 100\n",
    "N_inputs = train_images.shape[1]\n",
    "N_outputs = train_labels.shape[1]\n",
    "N_hidden_layer = 2\n",
    "learning_rate = 1e-3\n",
    "adaption_rate = 0.9\n",
    "\n",
    "network = Network()\n",
    "network.layer_lst.append(DenseLayer(N_inputs, N_inputs, ReLU, D_ReLU))\n",
    "network.layer_lst.extend([DenseLayer(N_inputs, N_inputs, ReLU, D_ReLU) for _ in range(N_hidden_layer)])\n",
    "network.layer_lst.append(DenseLayer(N_inputs, N_outputs, SoftMax, D_SoftMax))\n",
    "network.fit(train_images, train_labels, learning_rate, adaption_rate, N_epochs, N_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.9%\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = network.predict(test_images)\n",
    "accuracy = np.sum(np.argmax(predicted_labels, axis=1) == np.argmax(test_labels, axis=1)) / predicted_labels.shape[0] * 100\n",
    "print(\"Accuracy: {:.1f}%\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
