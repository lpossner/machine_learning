{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, RocCurveDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/nlp-getting-started/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/nlp-getting-started/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      Just happened a terrible car crash\n",
       "1       Heard about #earthquake is different cities, s...\n",
       "2       there is a forest fire at spot pond, geese are...\n",
       "3                Apocalypse lighting. #Spokane #wildfires\n",
       "4           Typhoon Soudelor kills 28 in China and Taiwan\n",
       "                              ...                        \n",
       "3258    EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...\n",
       "3259    Storm in RI worse than last hurricane. My city...\n",
       "3260    Green Line derailment in Chicago http://t.co/U...\n",
       "3261    MEG issues Hazardous Weather Outlook (HWO) htt...\n",
       "3262    #CityofCalgary has activated its Municipal Eme...\n",
       "Name: text, Length: 3263, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.loc[:, \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL = f\"DunnBC22/distilbert-base-uncased-Tweet_About_Disaster_Or_Not\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "# config = AutoConfig.from_pretrained(MODEL)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = 42\n",
    "# samples = 3000\n",
    "\n",
    "# texts = train_df.loc[:samples, \"text\"].to_list()\n",
    "# encoded_input = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "# output = model(**encoded_input)\n",
    "\n",
    "# y_pred = torch.argmax(output.logits, dim=1).detach().numpy()\n",
    "# probs = softmax(output.logits.detach().numpy(), axis=1)\n",
    "# y_test = train_df.loc[:samples, 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision = precision_score(y_test, y_pred)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# recall = recall_score(y_test, y_pred)\n",
    "# f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# print(f\"Precision: {precision}\")\n",
    "# print(f\"Accuracy: {accuracy}\")\n",
    "# print(f\"Recall: {recall}\")\n",
    "# print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# RocCurveDisplay.from_predictions(y_test, probs[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/language_processing/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9967854022979736}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "MODEL_NAME = 'hannybal/disaster-twitter-xlm-roberta-al'\n",
    "\n",
    "classifier = pipeline('text-classification', model=MODEL_NAME, tokenizer='cardiffnlp/twitter-xlm-roberta-base')\n",
    "classifier('I can see fire and smoke from the nearby fire!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 1000\n",
    "\n",
    "texts = train_df.loc[:samples, \"text\"].to_list()\n",
    "\n",
    "classes = classifier(texts)\n",
    "\n",
    "y_pred = [int(dct['label'] == 'LABEL_1') for dct in classes]\n",
    "y_test = train_df.loc[:samples, 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8717948717948718\n",
      "Accuracy: 0.7532467532467533\n",
      "Recall: 0.22295081967213115\n",
      "F1 Score: 0.35509138381201044\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "language_processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
