{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lpossner/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"knowledgator/biomed_NER\")\n",
    "df = ds[\"train\"].to_pandas()\n",
    "dct = df.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = r'[.,!@#$%&*(){}[\\]:;\\'\"<>?/_+-=|\\\\~`]'\n",
    "\n",
    "def clean_text(text):\n",
    "    return re.sub(chars, '', text).strip()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for ((_, text), (_, labels)) in zip(dct[\"text\"].items(), dct[\"entities\"].items()):\n",
    "    indices = [(entity[\"start\"], entity[\"end\"]) for entity in labels.tolist()]\n",
    "    entities = [entity[\"class\"] for entity in labels.tolist()]\n",
    "    other_indices = [(0, 0)] + indices\n",
    "    other_indices = other_indices + [(len(text), len(text))]\n",
    "    other_indices = [(char_index_1[1], char_index_2[0]) for char_index_1, char_index_2 in zip(other_indices[:-1], other_indices[1:])]\n",
    "    other_entities = [\"OTHER\"] * len(other_indices)\n",
    "    indices = indices + other_indices\n",
    "    entities = entities + other_entities\n",
    "    indices, entities = zip(*sorted(zip(indices, entities), key=lambda x: x[0]))\n",
    "    words = [text[index[0]:index[1]] for index in indices]\n",
    "    sentences = [(word, entity) for word, entity in zip(words, entities)]\n",
    "    corpus.append(sentences)\n",
    "\n",
    "corpus = [[(clean_text(words[0]), words[1]) for words in sentences] for sentences in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corpus = []\n",
    "for sentences in corpus:\n",
    "    new_sentences = []\n",
    "    for words in sentences:\n",
    "        if words[1] == \"OTHER\":\n",
    "            tokenized_words = word_tokenize(words[0])\n",
    "            if tokenized_words:\n",
    "                new_words = [(word, \"OTHER\") for word in tokenized_words]\n",
    "                new_sentences.extend(new_words)\n",
    "        else:\n",
    "            new_sentences.append(words)\n",
    "    new_corpus.append(new_sentences)\n",
    "        \n",
    "corpus = new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "\n",
    "class HMM_NER:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.observations = []\n",
    "        self.start_prob = defaultdict(float)\n",
    "        self.trans_prob = defaultdict(lambda: defaultdict(float))\n",
    "        self.emit_prob = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "    def train(self, data):\n",
    "        # Count occurrences to calculate probabilities\n",
    "        start_counts = Counter()\n",
    "        trans_counts = defaultdict(Counter)\n",
    "        emit_counts = defaultdict(Counter)\n",
    "        state_counts = Counter()\n",
    "\n",
    "        for sentence in data:\n",
    "            prev_state = None\n",
    "            for word, state in sentence:\n",
    "                state_counts[state] += 1\n",
    "                emit_counts[state][word] += 1\n",
    "                if prev_state is None:\n",
    "                    start_counts[state] += 1\n",
    "                else:\n",
    "                    trans_counts[prev_state][state] += 1\n",
    "                prev_state = state\n",
    "\n",
    "        # Calculate initial probabilities\n",
    "        total_starts = sum(start_counts.values())\n",
    "        for state in start_counts:\n",
    "            self.start_prob[state] = start_counts[state] / total_starts\n",
    "\n",
    "        # Calculate transition probabilities\n",
    "        for state in trans_counts:\n",
    "            total_transitions = sum(trans_counts[state].values())\n",
    "            for next_state in trans_counts[state]:\n",
    "                self.trans_prob[state][next_state] = (\n",
    "                    trans_counts[state][next_state] / total_transitions\n",
    "                )\n",
    "\n",
    "        # Calculate emission probabilities\n",
    "        for state in emit_counts:\n",
    "            total_emissions = sum(emit_counts[state].values())\n",
    "            for word in emit_counts[state]:\n",
    "                self.emit_prob[state][word] = emit_counts[state][word] / total_emissions\n",
    "\n",
    "        self.states = list(state_counts.keys())\n",
    "        self.observations = list({word for sentence in data for word, _ in sentence})\n",
    "\n",
    "    def viterbi(self, sentence):\n",
    "        V = [{}]\n",
    "        path = {}\n",
    "\n",
    "        # Initialize the base cases (t == 0)\n",
    "        for state in self.states:\n",
    "            V[0][state] = self.start_prob[state] * self.emit_prob[state].get(\n",
    "                sentence[0], 1e-6\n",
    "            )\n",
    "            path[state] = [state]\n",
    "\n",
    "        # Run Viterbi for t > 0\n",
    "        for t in range(1, len(sentence)):\n",
    "            V.append({})\n",
    "            new_path = {}\n",
    "\n",
    "            for curr_state in self.states:\n",
    "                (prob, prev_state) = max(\n",
    "                    (\n",
    "                        V[t - 1][prev_state]\n",
    "                        * self.trans_prob[prev_state].get(curr_state, 1e-6)\n",
    "                        * self.emit_prob[curr_state].get(sentence[t], 1e-6),\n",
    "                        prev_state,\n",
    "                    )\n",
    "                    for prev_state in self.states\n",
    "                )\n",
    "\n",
    "                V[t][curr_state] = prob\n",
    "                new_path[curr_state] = path[prev_state] + [curr_state]\n",
    "\n",
    "            path = new_path\n",
    "\n",
    "        # Find the most probable state sequence\n",
    "        (prob, final_state) = max(\n",
    "            (V[len(sentence) - 1][state], state) for state in self.states\n",
    "        )\n",
    "        return path[final_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_ner = HMM_NER()\n",
    "hmm_ner.train(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ORGANISM', 'ORGANISM', 'FUNCTION', 'OTHER', 'CHEMICALS', 'OTHER', 'OTHER', 'ACTIVITY', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'ORGANISM', 'OTHER', 'OTHER', 'OTHER', 'ACTIVITY', 'OTHER', 'OTHER', 'ACTIVITY', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'CHEMICALS', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'CHEMICALS', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'ORGANISM', 'ORGANISM', 'OTHER', 'OTHER', 'ORGANISM', 'OTHER']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"\n",
    "Weed seed inactivation in soil mesocosms via biosolarization with mature compost and tomato processing waste amendments Biosolarization is a fumigation alternative that combines passive solar heating with amendment-driven soil microbial activity to temporarily create antagonistic soil conditions, such as elevated temperature and acidity, that can inactivate weed seeds and other pest propagules.\n",
    "\"\"\"\n",
    "\n",
    "sentence = sentence.strip().split(\" \")\n",
    "predicted_tags = hmm_ner.viterbi(sentence)\n",
    "print(predicted_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weed ORGANISM\n",
      "seed ORGANISM\n",
      "inactivation FUNCTION\n",
      "in OTHER\n",
      "soil CHEMICALS\n",
      "mesocosms OTHER\n",
      "via OTHER\n",
      "biosolarization ACTIVITY\n",
      "with OTHER\n",
      "mature OTHER\n",
      "compost OTHER\n",
      "and OTHER\n",
      "tomato ORGANISM\n",
      "processing OTHER\n",
      "waste OTHER\n",
      "amendments OTHER\n",
      "Biosolarization ACTIVITY\n",
      "is OTHER\n",
      "a OTHER\n",
      "fumigation ACTIVITY\n",
      "alternative OTHER\n",
      "that OTHER\n",
      "combines OTHER\n",
      "passive OTHER\n",
      "solar OTHER\n",
      "heating OTHER\n",
      "with OTHER\n",
      "amendment-driven OTHER\n",
      "soil CHEMICALS\n",
      "microbial OTHER\n",
      "activity OTHER\n",
      "to OTHER\n",
      "temporarily OTHER\n",
      "create OTHER\n",
      "antagonistic OTHER\n",
      "soil CHEMICALS\n",
      "conditions, OTHER\n",
      "such OTHER\n",
      "as OTHER\n",
      "elevated OTHER\n",
      "temperature OTHER\n",
      "and OTHER\n",
      "acidity, OTHER\n",
      "that OTHER\n",
      "can OTHER\n",
      "inactivate OTHER\n",
      "weed ORGANISM\n",
      "seeds ORGANISM\n",
      "and OTHER\n",
      "other OTHER\n",
      "pest ORGANISM\n",
      "propagules. OTHER\n"
     ]
    }
   ],
   "source": [
    "for word, tag in zip(sentence, predicted_tags):\n",
    "    print(word, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "language_processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
